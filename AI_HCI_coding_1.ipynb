{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbA2xshZQHO0"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "# from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r9wvvxwaQh0o"
      },
      "outputs": [],
      "source": [
        "# gen_api_key = userdata.get('gemini_api')\n",
        "gen_api_key = \"ENTER YOUR API KEY"\n",
        "genai.configure(api_key=gen_api_key)\n",
        "\n",
        "# Create the model config\n",
        "generation_config = {\n",
        "  \"temperature\": 1,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 512,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "# Create the generative model\n",
        "model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-flash\",\n",
        "  generation_config=generation_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bkZQQHh_W-53"
      },
      "outputs": [],
      "source": [
        "# prompts that trigger end of conversation\n",
        "exit_list = ['quit', 'bye', \"exit\", \"goodbye\"]\n",
        "def valid_prompt(prompt):\n",
        "  # if prompt is 'quit' or \"bye\"\n",
        "  if (prompt.lower() in exit_list):\n",
        "    return False\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3khNkzqNZIHL"
      },
      "outputs": [],
      "source": [
        "# function that requests input prompt\n",
        "def request_prompt():\n",
        "  prompt = input(\"Please enter the prompt\\n('quit' or 'bye' to exit): \")\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tMHxruzJQzOC"
      },
      "outputs": [],
      "source": [
        "# main function\n",
        "def main():\n",
        "  # maintains chat history and helps gemini recall previous prompts and responses\n",
        "  chat_session = model.start_chat(\n",
        "    history=[]\n",
        "  )\n",
        "\n",
        "  prompt = request_prompt()\n",
        "  while valid_prompt(prompt):\n",
        "    # print history\n",
        "    if prompt == 'history':\n",
        "      print(\"\\nHere is the list of your recent prompts:\")\n",
        "      for chat in chat_session.history:\n",
        "        if chat.role == \"user\":\n",
        "          print(chat.parts[0].text)\n",
        "      print()\n",
        "\n",
        "    # ask gemini model\n",
        "    else:\n",
        "      response = response = chat_session.send_message(prompt)\n",
        "      print(response.text)\n",
        "\n",
        "    # request prompt again as long as it is valid\n",
        "    prompt = request_prompt()\n",
        "\n",
        "  print(\"\\nGoodbye! :)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "abq8QiEUfy1E",
        "outputId": "af8195f9-1b45-4a3f-8010-3aa3bb0d69c7"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # execute main function\n",
        "  main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
